# 주간 학습 내용

https://velog.io/@tmddn0920/Frequency-Attention

https://velog.io/@tmddn0920/CASS

---

이를 바탕으로 **SegFormer Teacher + CNN / Mamba Student**에 대한 아이디어를 생각해 보았습니다.

# Idea 1. Frequency Attention

이미지를 **주파수 영역으로 변환(Fourier Transform)** 했을 때

- **저주파**: 큰 덩어리, 전체적인 구조, 전역적 맥락 → 배경·윤곽
- **고주파**: 경계, 디테일, 작은 변화 → 텍스처·엣지

이는 곧 딥러닝 네트워크에서
- **High Level Feature**: 의미 단위, 전역적 정보, semantic 구조 → 저주파와 유사
- **Low Level Feature**: 간단한 패턴, 엣지, 국소 구조 → 고주파와 유사

즉, 서로 유사한 구조를 담는다.

**SegFormer**는 Global Self-Attention 구조로 전역 정보 + 지역 정보에 모두 강하지만, **CNN**은 깊이가 얕거나 모델의 크기가 작을 때 국소적 패턴에 치우치고 전역 정보를 잘 담아내지 못한다.

따라서, CNN이 Teacher Model인 SegFormer와 같은 결과를 도출하기 위해서는 지식 증류 과정에서 전역 정보를 효과적으로 전달받아야 한다.

**가설: "전역 정보를 주파수를 바탕으로 한 Attention Map을 바탕으로 더 효과적으로 전달한다."**

---

## Fast-SCNN

대표적인 경량 CNN 모델인 Fast SCNN의 구조를 살펴보고, 해당 가설을 바탕으로 전략을 구성한다.

## 1. 기본 전략

해상도를 맞추기 위해 **고속 푸리에 변환**을 활용한다.

**Encoder (Downsampling & Global Context)**
- 입력 이미지를 빠르게 축소하여 전역 문맥 정보를 추출

개선 방법: **저주파 성분(전역 구조, Semantic) 을 강조하는 Attention Map**을 **Encoder**에 적용
- 이를 통해 Student(Fast-SCNN)가 Teacher(SegFormer)의 전역적 표현을 더 잘 모방하도록 유도
- Encoder 단계에서는 **LPF(Low-Pass Filter) 기반 KD**

**Decoder (Feature Fusion & Upsampling)**
- Encoder의 전역 Feature와 초기 단계의 Low Level Feature 융합 → Segmentation Mask

개선 방법: **고주파 성분(경계, 디테일, 텍스처) 을 강조하는 Attention Map**을 **Decoder**에 적용
- 이를 통해 경량 CNN이 잃기 쉬운 세밀한 경계 정보를 Teacher로부터 효과적으로 전달
- Decoder 단계에서는 **HPF(High-Pass Filter) 기반 KD**

기대 효과: SegFormer 수준의 표현력을 일부 흡수 → 실시간성 + 정확도 확보

## 2. 역전 전략

**Encoder에 HPF 적용** → 저해상도 Feature에서도 경계·고주파 강조
**Decoder에 LPF 적용** → 복원 과정에서 전역 Semantic 강조

## 3. 변형 전략

1. Teacher에만 HPF + LPF
2. Student에만 HPF + LPF
3. Teacher + Student 모두 HPF + LPF

한 가지의 주파수 필터만 적용하는 게 아닌, 하이퍼파라미터를 지정해서 LPF, HPF의 비율을 조정하는 방식으로도 활용할 수 있다.

---

손실 함수의 예시를 GPT와 함께 만들어 보았다.

**(1) 기본 전략**

![](https://velog.velcdn.com/images/tmddn0920/post/1d623b9c-d159-46e5-8a7b-aba1c3864e5d/image.png)

**(2) Student 모델에만 필터 적용**

![](https://velog.velcdn.com/images/tmddn0920/post/330761a9-c0f8-427d-8dc2-ff4908189d8a/image.png)

**(3) 하이퍼파라미터를 바탕으로 LBF, HPF의 비율 조정**

![](https://velog.velcdn.com/images/tmddn0920/post/ac3bd8dc-42a6-4821-a83b-7b81e4670314/image.png)

**최종 손실 함수**

![](https://velog.velcdn.com/images/tmddn0920/post/7f4691f5-3644-4d88-b18e-5280ce4a6227/image.png)

---

## Mamba

Student 모델에 CNN 대신 **Mamba**를 사용했을 때를 생각해보자.

**SegFormer**
- 4-Stage 계층형 Transformer 백본 (출력 해상도: 1/4, 1/8, 1/16, 1/32)
- 초기 스테이지 (1/4, 1/8)
  - 얕은 Self-Attention과 Patch Embedding을 거쳐 로컬 패턴 + 준-글로벌 정보를 함께 포착
  - 엣지, 텍스처 같은 저수준 정보 + 부분적 전역 맥락
- 후기 스테이지 (1/16, 1/32)
  - 깊은 Transformer 블록을 통해 의미적/Semantic 단위 표현 학습
  - 객체 단위, 전역 맥락, 카테고리 레벨 구분 강화

**Mamba (VMamba-Tiny/Small)**
- 마찬가지로 4-Stage 피라미드 구조 (1/4~1/32)
- 초기 스테이지 (1/4, 1/8)
  - State Space Model(SSM) 블록 + DW-Conv로 국소적/단기 의존성과 디테일을 표현하는 데 강점
  - CNN과 비슷하게 경계·고주파적 성격이 강함
- 후기 스테이지 (1/16, 1/32)
  - SSM이 긴 시퀀스 문맥을 점차 통합 → 전역적 맥락 강화
  - 저주파적, Semantic 정보 강화
  
둘 다 4단계 멀티스케일 피라미드 → Teacher–Student 증류 시 스테이지 1:1 대응 가능

**전역 정보 학습 시점**
- SegFormer은 초기, Mamba는 후기부터

## 1. 기본 전략

해상도를 맞추기 위해 **고속 푸리에 변환**을 활용한다.

앞 2개 스테이지(1/4, 1/8) = HPF, 뒤 2개 스테이지(1/16, 1/32) = LPF
즉, 초·중기에는 경계/디테일(고주파), 후기에는 전역/Semantic(저주파) 증류를 활용한다.

## 2. 역전 전략

반대로 적용 (초기에 LPF, 후기에 HPF)
즉, 초·중기에는 전역/Semantic(저주파), 후기에는 경계/디테일(고주파) 증류를 활용한다.

## 3. 변형 전략

**1. 필터 적용**

- Teacher에만 HPF + LPF
- Student에만 HPF + LPF
- Teacher + Student 모두 HPF + LPF

**2. 스테이지 분할**

- 앞 2개, 뒤 2개로 나누는 게 아닌 앞 1개, 뒤 3개 or 앞 3개, 뒤 1개 등 다양한 변형 가능
- 하이퍼파라미터 지정 + 스테이지 별 HPF, LPF의 비율 조정

---

손실 함수의 예시를 GPT와 함께 만들어 보았다.

**(1) 기본 전략**

![](https://velog.velcdn.com/images/tmddn0920/post/cc18bcf0-b804-4cd2-adfb-5197005e3663/image.png)

![](https://velog.velcdn.com/images/tmddn0920/post/6b760622-86d1-4f6a-be02-3dc3f9df1a12/image.png)

**(2) 역전 전략**

![](https://velog.velcdn.com/images/tmddn0920/post/5635e774-2cfc-400c-af12-06d20576cb91/image.png)

**(3) Teacher 모델에만 필터 적용**

![](https://velog.velcdn.com/images/tmddn0920/post/359e9806-bc12-49be-8c71-1ab2ff83e2a0/image.png)

**(4) 스테이지 별 HPF, LPF의 비율 조정**

![](https://velog.velcdn.com/images/tmddn0920/post/6a9b4f23-29a7-41e2-bad6-774527b550ca/image.png)

**최종 손실 함수**

![](https://velog.velcdn.com/images/tmddn0920/post/e7fa4404-74eb-4193-87fa-a77f655432ab/image.png)

---

# Idea 2. Object-Aware Attention Distillation

SegFormer은 **Hierarchical ViT** 구조이다.
즉, CASS 논문의 내용에서 ViT의 구조를 **단순화(Layer Normalization 후 Self-Attention 적용)**하는 방식을 사용할 수 있다.

Self-Attention의 출력은 Patch-Level에서 공간적 + 의미적 관계가 함께 반영된 표현을 담는다.

CNN은 모델의 크기가 작을 때 전역 정보를 잘 담아내지 못하는 특성이 있는데, Self-Attention의 출력을 Distillation에 활용할 수 있다면 좋은 대안이 될 수 있다.

하지만, CNN 모델은 ViT를 활용하지 않는다. 즉, 비교할 수 있는 **같은 차원의 Attention Map**을 뽑아내는 데 한계가 있다.

### Student 모델에서 유사한 Self-Attention Map 얻기

- Similarity-Based Proxy
- Query and Key 활용
- Student가 Mamba일 때 State-Affinity

> 어떤 방법을 쓸 수 있을 지 검색을 해 보았는데, 다음과 같은 방법들이 나왔습니다!
각 방법들에 대해서는 아직 공부가 필요할 거 같습니다.

### Spectral Object-Level Context Distillation

같은 차원의 Map을 얻은 상태에서, Spectral Object-Level Context Distillation 사용을 고려해볼 수 있다.

SegFormer은 Multi-Head Attention Graph이다. 따라서 Head 별로 고유값 분해를 했을 때 다른 스펙트럼을 얻을 수 있다.

> 일단 Student 모델은 Multi-Head가 아니라고 가정해 보았습니다!

1. Teacher의 모든 Head Eigenvalue(고유값)을 평균 → Distillation
2. Teacher Head 중 Object-Context에 강한 Head만 선택

결론적으로, 다음과 같은 2가지 방법을 고려해 볼 수 있을 거 같다.

> 혹은 여기서 스펙트럼이 아니라 `Graph Laplacian: L = D − A` 이라는 식을 바탕으로 얻은 스펙트럼은 곧 주파수와 비슷한 역할을 할 수 있다는 내용이 있어서 조금 더 알아보면 좋을 거 같습니다.

### Distilling Spectral Graph

Attention Graph의 고유값 분포에서 상위 성분은 객체 단위의 구조를 반영하지만, 하위 성분은 노이즈/세부적 잡음에 해당한다.

따라서 **Low-Rank Approximation**을 통해 중요한 성분만 남겨 객체 단위 정보는 강화 + 불필요한 세부는 억제한다.

이를 위해 **Teacher Attention Graph를 고유분해**한 후 큰 고유값은 증폭, 작은 고유값은 감소시킨다.

그 후 Distillation 과정에서 결국 우리가 비교하는 건 **입력 이미지에서 유도된 패치 관계**이기 때문에 **모델 구조와 관계없이 비교 / 정렬**이 가능하다.

---
